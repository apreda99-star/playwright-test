{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apreda99-star/playwright-test/blob/main/visual_product_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eafd6bf2",
      "metadata": {
        "id": "eafd6bf2"
      },
      "source": [
        "## üì¶ Step 1: Importazione Librerie\n",
        "\n",
        "Importiamo tutte le librerie necessarie per il progetto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LQprEOhq7zyB",
      "metadata": {
        "id": "LQprEOhq7zyB"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Seleziona il file dal tuo PC\n",
        "MODEL_PATH = \"resnet50_places365.pth.tar\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "512d72df",
      "metadata": {
        "id": "512d72df"
      },
      "outputs": [],
      "source": [
        "# Librerie principali\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Utilit√†\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"‚úì Librerie importate con successo!\")\n",
        "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úì CUDA disponibile: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5d130d3",
      "metadata": {
        "id": "a5d130d3"
      },
      "source": [
        "## ‚öôÔ∏è Step 2: Configurazione Parametri\n",
        "\n",
        "Definiamo tutti i parametri dell'esperimento.\n",
        "\n",
        "**Parametri principali:**\n",
        "- `MODEL_PATH`: Path al modello pre-addestrato\n",
        "- `DATASET_PATH`: Path al dataset Places365\n",
        "- `BATCH_SIZE`: Numero di immagini per batch\n",
        "- `NUM_EPOCHS`: Numero di epoche per il fine-tuning\n",
        "- `LEARNING_RATE`: Tasso di apprendimento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8be3891d"
      },
      "source": [
        "Verifichiamo in dettaglio la configurazione CUDA di PyTorch:"
      ],
      "id": "8be3891d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37768af3"
      },
      "source": [
        "import torch\n",
        "\n",
        "print(f\"CUDA disponibile: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Nome GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Numero di GPU: {torch.cuda.device_count()}\")\n",
        "    print(f\"Capacit√† CUDA: {torch.cuda.get_device_capability(0)}\")\n",
        "    print(f\"Memoria totale GPU: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(\"Nessuna GPU CUDA rilevata. Assicurati che PyTorch sia installato con supporto CUDA e che i driver della GPU siano aggiornati.\")\n",
        "\n",
        "print(f\"Variabile DEVICE impostata a: {DEVICE}\")"
      ],
      "id": "37768af3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4acb70b7",
      "metadata": {
        "id": "4acb70b7"
      },
      "outputs": [],
      "source": [
        "# Path al modello\n",
        "# Per Google Colab (dopo aver eseguito il download):\n",
        "MODEL_PATH = \"resnet50_places365.pth.tar\"\n",
        "\n",
        "# Per uso locale su Windows (decommenta se usi VS Code):\n",
        "# MODEL_PATH = r\"C:\\Users\\preda\\Downloads\\resnet50_places365.pth.tar\"\n",
        "\n",
        "# Path al dataset\n",
        "DATASET_PATH = \"places365_standard\"  # cartella con sottocartelle train/val\n",
        "\n",
        "# Parametri di training\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_WORKERS = 2  # Ridotto per Colab (usa 4 se hai GPU locale)\n",
        "\n",
        "# Device (GPU o CPU)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Stampa configurazione\n",
        "print(\"=\" * 60)\n",
        "print(\"CONFIGURAZIONE ESPERIMENTO\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Epoche fine-tuning: {NUM_EPOCHS}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Num workers: {NUM_WORKERS}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cf4cc1c",
      "metadata": {
        "id": "9cf4cc1c"
      },
      "source": [
        "## üîß Step 3: Caricamento Modello Pre-addestrato\n",
        "\n",
        "Carichiamo ResNet50 con i pesi pre-addestrati su Places365.\n",
        "\n",
        "**Nota:** Se non hai il file, scaricalo da:\n",
        "- URL: http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec18da36",
      "metadata": {
        "id": "ec18da36"
      },
      "outputs": [],
      "source": [
        "# SOLO PER GOOGLE COLAB - Scarica e organizza il dataset\n",
        "# Decommenta le righe seguenti se usi Colab:\n",
        "# SOLO PER GOOGLE COLAB - Scarica e organizza il dataset\n",
        "\n",
        "# Step 1: Download validation set\n",
        "!wget http://data.csail.mit.edu/places/places365/val_256.tar\n",
        "!tar -xf val_256.tar\n",
        "!mkdir -p places365_standard/val\n",
        "!mv val_256/* places365_standard/val/ 2>/dev/null || mv val/* places365_standard/val/ 2>/dev/null\n",
        "!rm -rf val_256 val\n",
        "print(\"‚úì Immagini estratte in places365_standard/val/\")\n",
        "\n",
        "# Step 2: Download file delle categorie\n",
        "!wget https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt\n",
        "!wget http://data.csail.mit.edu/places/places365/filelist_places365-standard.tar\n",
        "!tar -xf filelist_places365-standard.tar\n",
        "print(\"‚úì File delle categorie scaricati\")\n",
        "\n",
        "# Step 3: Organizza le immagini in sottocartelle per classe\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Leggi il file delle categorie\n",
        "with open('categories_places365.txt', 'r') as f:\n",
        "    categories = [line.strip().split(' ')[0] for line in f]\n",
        "\n",
        "print(f\"Trovate {len(categories)} categorie\")\n",
        "\n",
        "# Crea le sottocartelle per ogni classe\n",
        "for category in categories:\n",
        "    category_path = os.path.join('places365_standard/val', category.lstrip('/').replace('/', '_'))\n",
        "    os.makedirs(category_path, exist_ok=True)\n",
        "\n",
        "# Leggi il file che mappa le immagini alle classi\n",
        "val_file = 'filelist_places365-standard/places365_val.txt'\n",
        "if os.path.exists(val_file):\n",
        "    with open(val_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    print(f\"Organizzazione di {len(lines)} immagini in {len(categories)} classi...\")\n",
        "\n",
        "    # Sposta ogni immagine nella sua sottocartella\n",
        "    for line in tqdm(lines, desc=\"Organizing images\"):\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        # Format: val/airfield/Places365_val_00000001.jpg 0\n",
        "        img_name = parts[0]\n",
        "        class_idx = int(parts[1])\n",
        "\n",
        "        full_category_path = categories[class_idx]\n",
        "        class_dir_name = full_category_path.lstrip('/').replace('/', '_')\n",
        "\n",
        "        # Percorsi sorgente e destinazione\n",
        "        src = os.path.join('places365_standard/val', img_name)\n",
        "        dst = os.path.join('places365_standard/val', class_dir_name, img_name)\n",
        "\n",
        "        # Sposta il file se esiste\n",
        "        if os.path.exists(src):\n",
        "            shutil.move(src, dst)\n",
        "\n",
        "    print(\"‚úì Organizzazione completata!\")\n",
        "\n",
        "    # Verifica risultato\n",
        "    subdirs = [d for d in os.listdir('places365_standard/val') if os.path.isdir(os.path.join('places365_standard/val', d))]\n",
        "    print(f\"‚úì Numero di classi create: {len(subdirs)}\")\n",
        "\n",
        "    # Conta immagini per alcune classi\n",
        "    for category in categories[:3]:\n",
        "        cat_name_for_dir = category.lstrip('/').replace('/', '_')\n",
        "        cat_path = os.path.join('places365_standard/val', cat_name_for_dir)\n",
        "        if os.path.exists(cat_path):\n",
        "            num_imgs = len([f for f in os.listdir(cat_path) if f.endswith('.jpg')])\n",
        "            print(f\"  - {cat_name_for_dir}: {num_imgs} immagini\")\n",
        "\n",
        "#Scarica il modello se non esiste\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"‚ö† ATTENZIONE: File {MODEL_PATH} non trovato! Scaricamento in corso...\")\n",
        "    !wget http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar\n",
        "    print(f\"‚úì {MODEL_PATH} scaricato con successo!\")\n",
        "\n",
        "#Verifica esistenza del modello\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"‚ùå ERRORE: Impossibile scaricare {MODEL_PATH}. Controlla la connessione o l'URL.\")\n",
        "else:\n",
        "    # Carica checkpoint\n",
        "    print(f\"Caricamento checkpoint da {MODEL_PATH}...\")\n",
        "    checkpoint = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
        "\n",
        "    # Crea modello ResNet50 con 365 classi (Places365)\n",
        "    model = models.resnet50(num_classes=365)\n",
        "\n",
        "    # Rimuovi il prefisso \"module.\" dai nomi dei layer\n",
        "    state_dict = {k.replace(\"module.\", \"\"): v for k, v in checkpoint[\"state_dict\"].items()}\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    # Sposta il modello sul device\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    print(f\"‚úì Modello caricato con successo!\")\n",
        "    print(f\"‚úì Numero di classi: 365 (Places365)\")\n",
        "    print(f\"‚úì Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbae30d9",
      "metadata": {
        "id": "fbae30d9"
      },
      "source": [
        "## üìÅ Step 4: Preparazione Dataset\n",
        "\n",
        "Prepariamo il dataset Places365 con le trasformazioni appropriate.\n",
        "\n",
        "**Trasformazioni applicate:**\n",
        "1. Resize a 256x256\n",
        "2. Center crop a 224x224\n",
        "3. Conversione a Tensor\n",
        "4. Normalizzazione (mean e std di ImageNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57bb9d1f",
      "metadata": {
        "id": "57bb9d1f"
      },
      "outputs": [],
      "source": [
        "# Trasformazioni per le immagini\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Path ai dataset\n",
        "val_path = os.path.join(DATASET_PATH, \"val\") # Corretto per puntare direttamente alla cartella 'val'\n",
        "train_path = os.path.join(DATASET_PATH, \"train\") # Mantiene il path originale per il training set\n",
        "\n",
        "print(f\"Path validation: {val_path}\")\n",
        "print(f\"Path training: {train_path}\")\n",
        "\n",
        "# Verifica esistenza\n",
        "if os.path.exists(val_path):\n",
        "    print(f\"‚úì Trovata cartella validation\")\n",
        "    # Conta le sottocartelle (classi) se esiste\n",
        "    if os.path.isdir(val_path):\n",
        "        subdirs = [d for d in os.listdir(val_path) if os.path.isdir(os.path.join(val_path, d))]\n",
        "        # Controlla se la lista di subdirs √® vuota, il che indicherebbe che le immagini sono direttamente nella cartella\n",
        "        if not subdirs:\n",
        "            print(f\"‚úì Nessuna sottocartella di classe trovata direttamente in {val_path}. Le immagini dovrebbero essere qui.\")\n",
        "        else:\n",
        "            print(f\"‚úì Numero di classi trovate: {len(subdirs)}\")\n",
        "else:\n",
        "    print(f\"‚ö† Cartella validation non trovata. Verifica la struttura dei file.\")\n",
        "\n",
        "if os.path.exists(train_path):\n",
        "    print(f\"‚úì Trovata cartella training\")\n",
        "else:\n",
        "    print(f\"‚ö† Cartella training non trovata (user√≤ validation set per il training)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"üßπ Pulizia e riorganizzazione validation set...\")\n",
        "print()\n",
        "\n",
        "# Step 1: Rimuovi tutto il vecchio\n",
        "!rm -rf places365_standard/val\n",
        "!rm -f val_256.tar categories_places365.txt places365_val.txt\n",
        "!rm -rf filelist_places365-standard*\n",
        "print(\"‚úì File vecchi rimossi\")\n",
        "print()\n",
        "\n",
        "# Step 2: Download validation set\n",
        "print(\"üì• Download validation set...\")\n",
        "!wget -q http://data.csail.mit.edu/places/places365/val_256.tar\n",
        "!tar -xf val_256.tar\n",
        "!mkdir -p places365_standard/val\n",
        "!mv val_256/* places365_standard/val/ 2>/dev/null || mv val/* places365_standard/val/ 2>/dev/null\n",
        "!rm -rf val_256 val\n",
        "print(\"‚úì Immagini estratte in places365_standard/val/\")\n",
        "print()\n",
        "\n",
        "# Step 3: Download file delle categorie\n",
        "print(\"üì• Download file delle categorie...\")\n",
        "!wget -q https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt\n",
        "!wget -q http://data.csail.mit.edu/places/places365/filelist_places365-standard.tar\n",
        "!tar -xf filelist_places365-standard.tar\n",
        "print(\"‚úì File delle categorie scaricati\")\n",
        "print()\n",
        "\n",
        "# Step 4: Organizza le immagini in sottocartelle per classe\n",
        "print(\"üìÅ Organizzazione immagini in 365 classi...\")\n",
        "\n",
        "# Leggi il file delle categorie\n",
        "with open('categories_places365.txt', 'r') as f:\n",
        "    categories = [line.strip().split(' ')[0] for line in f]\n",
        "\n",
        "print(f\"Trovate {len(categories)} categorie\")\n",
        "\n",
        "# Crea le sottocartelle per ogni classe\n",
        "for category in categories:\n",
        "    # Use the full category path, replacing slashes for unique directory names\n",
        "    class_dir_name = category.lstrip('/').replace('/', '_') # e.g., 'a_airfield' from '/a/airfield'\n",
        "    category_path = os.path.join('places365_standard/val', class_dir_name)\n",
        "    os.makedirs(category_path, exist_ok=True)\n",
        "\n",
        "# Cerca il file places365_val.txt\n",
        "val_file = None\n",
        "possible_paths = [\n",
        "    'filelist_places365-standard/places365_val.txt',\n",
        "    'places365_val.txt',\n",
        "    'val.txt'\n",
        "]\n",
        "\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        val_file = path\n",
        "        print(f\"‚úì Trovato file mapping: {path}\")\n",
        "        break\n",
        "\n",
        "if val_file is None:\n",
        "    # Verifica cosa c'√® nella cartella estratta\n",
        "    if os.path.exists('filelist_places365-standard'):\n",
        "        files = os.listdir('filelist_places365-standard')\n",
        "        print(f\"File estratti: {files}\")\n",
        "        # Cerca file con 'val' nel nome\n",
        "        for f in files:\n",
        "            if 'val' in f.lower():\n",
        "                val_file = os.path.join('filelist_places365-standard', f)\n",
        "                print(f\"‚úì Trovato: {val_file}\")\n",
        "                break\n",
        "\n",
        "if val_file is None or not os.path.exists(val_file):\n",
        "    print(\"‚ùå ERRORE: Impossibile trovare il file di mapping!\")\n",
        "    print(\"   Il tar √® stato estratto ma il file places365_val.txt non √® presente.\")\n",
        "    print(\"   Controlla il contenuto della cartella filelist_places365-standard/\")\n",
        "else:\n",
        "    with open(val_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    print(f\"Organizzazione di {len(lines)} immagini...\")\n",
        "\n",
        "    # Sposta ogni immagine nella sua sottocartella\n",
        "    moved_count = 0\n",
        "    for line in tqdm(lines, desc=\"Organizing\"):\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        img_name = parts[0]\n",
        "        class_idx = int(parts[1])\n",
        "\n",
        "        if class_idx >= len(categories) or class_idx < 0:\n",
        "            print(f\"‚ö†Ô∏è Errore: Indice di classe {class_idx} fuori dai limiti per l'immagine {img_name}. Saltato.\")\n",
        "            continue\n",
        "\n",
        "        full_category_path = categories[class_idx] # e.g., '/a/airfield'\n",
        "\n",
        "        # Use the same logic as for folder creation to get the unique directory name\n",
        "        class_dir_name = full_category_path.lstrip('/').replace('/', '_') # e.g., 'a_airfield'\n",
        "\n",
        "        # Percorsi sorgente e destinazione\n",
        "        src = os.path.join('places365_standard/val', img_name)\n",
        "        dst = os.path.join('places365_standard/val', class_dir_name, img_name)\n",
        "\n",
        "        # Sposta il file se esiste\n",
        "        if os.path.exists(src):\n",
        "            os.makedirs(os.path.dirname(dst), exist_ok=True) # Ensure destination directory exists\n",
        "            shutil.move(src, dst)\n",
        "            moved_count += 1\n",
        "\n",
        "    print(f\"‚úì Organizzazione completata! Spostate {moved_count} immagini.\")\n",
        "\n",
        "    # Verifica risultato\n",
        "    subdirs = [d for d in os.listdir('places365_standard/val') if os.path.isdir(os.path.join('places365_standard/val', d))]\n",
        "    print(f\"‚úì Numero di classi create: {len(subdirs)}\")\n",
        "\n",
        "    # Conta immagini per alcune classi\n",
        "    for category in categories[:3]: # Using original full category paths\n",
        "        cat_name_for_dir = category.lstrip('/').replace('/', '_') # Use the new unique name\n",
        "        cat_path = os.path.join('places365_standard/val', cat_name_for_dir)\n",
        "        if os.path.exists(cat_path):\n",
        "            num_imgs = len([f for f in os.listdir(cat_path) if f.endswith('.jpg')])\n",
        "            print(f\"  - {cat_name_for_dir}: {num_imgs} immagini\")"
      ],
      "metadata": {
        "id": "W6fK3LxhzyRn"
      },
      "id": "W6fK3LxhzyRn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66426045",
      "metadata": {
        "id": "66426045"
      },
      "outputs": [],
      "source": [
        "# Carica il validation set\n",
        "if not os.path.exists(val_path):\n",
        "    print(f\"‚ö† ATTENZIONE: Cartella {val_path} non trovata!\")\n",
        "    print(f\"Scarica il dataset Places365 da: http://places2.csail.mit.edu/download.html\")\n",
        "else:\n",
        "    print(f\"Caricamento validation set da {val_path}...\")\n",
        "    valset = datasets.ImageFolder(val_path, transform=transform)\n",
        "    valloader = DataLoader(\n",
        "        valset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"‚úì Validation set caricato!\")\n",
        "    print(f\"‚úì Numero di immagini: {len(valset)}\")\n",
        "    print(f\"‚úì Numero di classi: {len(valset.classes)}\")\n",
        "    print(f\"‚úì Numero di batch: {len(valloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duNRmM0vYZFk",
      "metadata": {
        "id": "duNRmM0vYZFk"
      },
      "outputs": [],
      "source": [
        "# DEBUG: Verifica cosa c'√® in val/\n",
        "import os\n",
        "\n",
        "print(\"üîç Analisi dettagliata:\")\n",
        "print()\n",
        "\n",
        "if os.path.exists(\"places365_standard/val\"):\n",
        "    val_contents = os.listdir(\"places365_standard/val\")\n",
        "    dirs = [d for d in val_contents if os.path.isdir(os.path.join(\"places365_standard/val\", d))]\n",
        "    files = [f for f in val_contents if os.path.isfile(os.path.join(\"places365_standard/val\", f))]\n",
        "\n",
        "    print(f\"Sottocartelle in val/: {len(dirs)}\")\n",
        "    print(f\"File in val/: {len(files)}\")\n",
        "    print()\n",
        "\n",
        "    if len(dirs) == 0 and len(files) > 0:\n",
        "        print(\"‚ö†Ô∏è PROBLEMA CONFERMATO: Le immagini sono direttamente in val/ invece che in sottocartelle\")\n",
        "        print(\"   SOLUZIONE: Elimina la cartella val e ri-esegui la cella 8\")\n",
        "        print()\n",
        "        print(\"   Esegui questi comandi:\")\n",
        "        print(\"   !rm -rf places365_standard/val\")\n",
        "        print(\"   !rm -f val_256.tar categories_places365.txt\")\n",
        "        print(\"   !rm -rf filelist_places365-standard*\")\n",
        "        print(\"   Poi ri-esegui la cella 8\")\n",
        "    elif len(dirs) == 365:\n",
        "        print(\"‚úÖ TUTTO OK! Il validation set √® organizzato correttamente\")\n",
        "        print(f\"   Prime 5 classi: {sorted(dirs)[:5]}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è PROBLEMA: Trovate {len(dirs)} sottocartelle invece di 365\")\n",
        "\n",
        "if os.path.exists(\"places365_standard/train\"):\n",
        "    train_contents = os.listdir(\"places365_standard/train\")\n",
        "    train_dirs = [d for d in train_contents if os.path.isdir(os.path.join(\"places365_standard/train\", d))]\n",
        "    print(f\"\\n‚úÖ Training set OK: {len(train_dirs)} classi trovate\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JU4fYbjUbZLL",
      "metadata": {
        "id": "JU4fYbjUbZLL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce6aad60",
      "metadata": {
        "id": "ce6aad60"
      },
      "outputs": [],
      "source": [
        "# Carica il training set\n",
        "if os.path.exists(train_path):\n",
        "    print(f\"Caricamento training set da {train_path}...\")\n",
        "    trainset_full = datasets.ImageFolder(train_path, transform=transform)\n",
        "\n",
        "    # USA SOLO IL 10% DEL TRAINING SET (per velocizzare)\n",
        "    from torch.utils.data import Subset\n",
        "    import numpy as np\n",
        "\n",
        "    subset_percentage = 0.1  # 10% del training set\n",
        "    subset_size = int(subset_percentage * len(trainset_full))\n",
        "    indices = np.random.choice(len(trainset_full), size=subset_size, replace=False)\n",
        "    trainset = Subset(trainset_full, indices)\n",
        "\n",
        "    trainloader = DataLoader(\n",
        "        trainset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "    print(f\"‚úì Training set caricato!\")\n",
        "    print(f\"‚úì Dataset completo: {len(trainset_full):,} immagini\")\n",
        "    print(f\"‚úì Usando subset {subset_percentage*100:.0f}%: {len(trainset):,} immagini\")\n",
        "    print(f\"‚ö†Ô∏è  Per usare tutto il dataset, modifica subset_percentage = 1.0\")\n",
        "else:\n",
        "    print(f\"‚ö† Training set non trovato. User√≤ il validation set per il fine-tuning.\")\n",
        "    print(f\"   (Questo va bene per esperimenti rapidi)\")\n",
        "    trainloader = valloader\n",
        "    trainset = valset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "211141f7",
      "metadata": {
        "id": "211141f7"
      },
      "source": [
        "## üìä Step 5: Valutazione Iniziale (Prima del Fine-Tuning)\n",
        "\n",
        "Calcoliamo la **loss** e l'**accuracy** del modello pre-addestrato sul validation set, **senza** fare alcun fine-tuning.\n",
        "\n",
        "Questo ci servir√† come baseline per confrontare i risultati dopo il fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d10b50ca",
      "metadata": {
        "id": "d10b50ca"
      },
      "outputs": [],
      "source": [
        "# Funzione di loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Modalit√† evaluation (disabilita dropout, batch norm, ecc.)\n",
        "model.eval()\n",
        "\n",
        "total_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "print(\"Valutazione modello PRE-ADDESTRATO in corso...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Disabilita il calcolo dei gradienti per velocizzare\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (images, labels) in enumerate(tqdm(valloader, desc=\"Evaluating\")):\n",
        "        # Sposta i dati sul device (GPU/CPU)\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Accumula statistiche\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "eval_time = time.time() - start_time\n",
        "loss_before = total_loss / len(valset)\n",
        "accuracy_before = 100.0 * correct / total\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"RISULTATI INIZIALI (modello pre-addestrato)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"‚úì Tempo di valutazione: {eval_time:.2f} secondi\")\n",
        "print(f\"‚úì Loss iniziale: {loss_before:.4f}\")\n",
        "print(f\"‚úì Accuracy iniziale: {accuracy_before:.2f}%\")\n",
        "print(f\"‚úì Immagini corrette: {correct}/{total}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49ebf2bf",
      "metadata": {
        "id": "49ebf2bf"
      },
      "source": [
        "## üéì Step 6: Configurazione Fine-Tuning\n",
        "\n",
        "Prepariamo il modello per il fine-tuning.\n",
        "\n",
        "**Due strategie possibili:**\n",
        "1. **Fine-tuning solo ultimo layer** (pi√π veloce, meno rischi di overfitting)\n",
        "2. **Fine-tuning completo** (pi√π lento, potenzialmente migliori risultati)\n",
        "\n",
        "Di default usiamo la strategia 1 (solo ultimo layer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee48712",
      "metadata": {
        "id": "bee48712"
      },
      "outputs": [],
      "source": [
        "# STRATEGIA 1: Fine-tuning solo ultimo layer (fc)\n",
        "print(\"Configurazione: Fine-tuning solo ultimo layer (fc)\")\n",
        "\n",
        "# Congela tutti i layer\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Sblocca solo l'ultimo layer\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Conta i parametri trainable\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"‚úì Parametri trainable: {trainable_params:,}\")\n",
        "print(f\"‚úì Parametri totali: {total_params:,}\")\n",
        "print(f\"‚úì Percentuale trainable: {100.0 * trainable_params / total_params:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f43a5c41",
      "metadata": {
        "id": "f43a5c41"
      },
      "outputs": [],
      "source": [
        "# Optimizer (Adam)\n",
        "optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "# Scheduler per ridurre automaticamente il learning rate\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',      # Monitora la loss (vogliamo minimizzarla)\n",
        "    factor=0.5,      # Riduci LR del 50%\n",
        "    patience=1       # Aspetta 1 epoca prima di ridurre\n",
        "    # verbose=True  # Rimosso, non pi√π supportato in alcune versioni di PyTorch\n",
        ")\n",
        "\n",
        "print(\"‚úì Optimizer configurato: Adam\")\n",
        "print(f\"‚úì Learning rate iniziale: {LEARNING_RATE}\")\n",
        "print(\"‚úì Scheduler configurato: ReduceLROnPlateau\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee84f5d",
      "metadata": {
        "id": "1ee84f5d"
      },
      "source": [
        "## üöÄ Step 7: Training (Fine-Tuning)\n",
        "\n",
        "Eseguiamo il fine-tuning del modello per `NUM_EPOCHS` epoche.\n",
        "\n",
        "**Cosa succede in ogni epoca:**\n",
        "1. Il modello processa tutti i batch del training set\n",
        "2. Per ogni batch: forward pass ‚Üí calcolo loss ‚Üí backward pass ‚Üí aggiornamento pesi\n",
        "3. Alla fine dell'epoca: calcolo loss e accuracy medie\n",
        "4. Il scheduler aggiusta il learning rate se necessario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66f1c0f0",
      "metadata": {
        "id": "66f1c0f0"
      },
      "outputs": [],
      "source": [
        "# Modalit√† training (abilita dropout, batch norm, ecc.)\n",
        "model.train()\n",
        "\n",
        "# Liste per salvare le metriche\n",
        "training_losses = []\n",
        "training_accuracies = []\n",
        "\n",
        "print(f\"\\nInizio fine-tuning per {NUM_EPOCHS} epoche...\\n\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Epoca {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(tqdm(trainloader, desc=f\"Epoch {epoch+1}\")):\n",
        "        # Sposta i dati sul device\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Azzera i gradienti\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumula statistiche\n",
        "        epoch_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # Calcola metriche dell'epoca\n",
        "    epoch_time = time.time() - start_time\n",
        "    avg_loss = epoch_loss / len(trainset)\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    # Salva metriche\n",
        "    training_losses.append(avg_loss)\n",
        "    training_accuracies.append(accuracy)\n",
        "\n",
        "    # Stampa risultati epoca\n",
        "    print(f\"\\nRisultati Epoca {epoch+1}:\")\n",
        "    print(f\"  ‚Ä¢ Tempo: {epoch_time:.2f}s\")\n",
        "    print(f\"  ‚Ä¢ Loss: {avg_loss:.4f}\")\n",
        "    print(f\"  ‚Ä¢ Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"  ‚Ä¢ Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    # Aggiorna learning rate con lo scheduler\n",
        "    scheduler.step(avg_loss)\n",
        "    print()\n",
        "\n",
        "print(f\"{'='*60}\")\n",
        "print(\"‚úì Fine-tuning completato!\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17a04c6d",
      "metadata": {
        "id": "17a04c6d"
      },
      "source": [
        "## üìä Step 8: Valutazione Finale (Dopo Fine-Tuning)\n",
        "\n",
        "Ricalcoliamo la **loss** e l'**accuracy** del modello dopo il fine-tuning.\n",
        "\n",
        "Questo ci permetter√† di confrontare i risultati con quelli ottenuti prima del fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b156d754",
      "metadata": {
        "id": "b156d754"
      },
      "outputs": [],
      "source": [
        "# Modalit√† evaluation\n",
        "model.eval()\n",
        "\n",
        "total_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "print(\"Valutazione modello FINE-TUNED in corso...\")\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (images, labels) in enumerate(tqdm(valloader, desc=\"Final Evaluation\")):\n",
        "        # Sposta i dati sul device\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Accumula statistiche\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "eval_time = time.time() - start_time\n",
        "loss_after = total_loss / len(valset)\n",
        "accuracy_after = 100.0 * correct / total\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"RISULTATI FINALI (modello fine-tuned)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"‚úì Tempo di valutazione: {eval_time:.2f} secondi\")\n",
        "print(f\"‚úì Loss finale: {loss_after:.4f}\")\n",
        "print(f\"‚úì Accuracy finale: {accuracy_after:.2f}%\")\n",
        "print(f\"‚úì Immagini corrette: {correct}/{total}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76691c14",
      "metadata": {
        "id": "76691c14"
      },
      "source": [
        "## üîç Step 9: Confronto Risultati\n",
        "\n",
        "Confrontiamo i risultati **prima** e **dopo** il fine-tuning per vedere se c'√® stato un miglioramento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612fd372",
      "metadata": {
        "id": "612fd372"
      },
      "outputs": [],
      "source": [
        "print(f\"{'='*60}\")\n",
        "print(\"CONFRONTO RISULTATI\")\n",
        "print(f\"{'='*60}\")\n",
        "print()\n",
        "\n",
        "# Confronto Loss\n",
        "print(\"üìâ LOSS:\")\n",
        "print(f\"  Prima del fine-tuning:  {loss_before:.4f}\")\n",
        "print(f\"  Dopo il fine-tuning:    {loss_after:.4f}\")\n",
        "print(f\"  Differenza:             {loss_before - loss_after:+.4f}\")\n",
        "print(f\"  Variazione %:           {((loss_before - loss_after) / loss_before * 100):+.2f}%\")\n",
        "print()\n",
        "\n",
        "# Confronto Accuracy\n",
        "print(\"üéØ ACCURACY:\")\n",
        "print(f\"  Prima del fine-tuning:  {accuracy_before:.2f}%\")\n",
        "print(f\"  Dopo il fine-tuning:    {accuracy_after:.2f}%\")\n",
        "print(f\"  Differenza:             {accuracy_after - accuracy_before:+.2f}%\")\n",
        "print()\n",
        "\n",
        "# Verdetto\n",
        "if loss_after < loss_before:\n",
        "    print(\"‚úÖ SUCCESSO! Il fine-tuning ha MIGLIORATO il modello.\")\n",
        "    improvement = ((loss_before - loss_after) / loss_before * 100)\n",
        "    print(f\"   Riduzione loss: {improvement:.2f}%\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Il fine-tuning ha PEGGIORATO il modello.\")\n",
        "    degradation = ((loss_after - loss_before) / loss_before * 100)\n",
        "    print(f\"   Aumento loss: {degradation:.2f}%\")\n",
        "\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc46c982",
      "metadata": {
        "id": "bc46c982"
      },
      "source": [
        "## üìà Step 10: Visualizzazione Grafici\n",
        "\n",
        "Creiamo dei grafici per visualizzare:\n",
        "1. **Loss** durante il training\n",
        "2. **Accuracy** durante il training\n",
        "3. **Confronto** prima vs dopo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5fd84ad",
      "metadata": {
        "id": "f5fd84ad"
      },
      "outputs": [],
      "source": [
        "# Grafico Loss e Accuracy durante training\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Grafico Loss\n",
        "axes[0].plot(range(1, NUM_EPOCHS+1), training_losses, marker='o', linewidth=2, label='Training Loss')\n",
        "axes[0].axhline(y=loss_before, color='r', linestyle='--', label='Loss iniziale', linewidth=2)\n",
        "axes[0].set_xlabel('Epoca', fontsize=12)\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].set_title('Training Loss durante Fine-Tuning', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Grafico Accuracy\n",
        "axes[1].plot(range(1, NUM_EPOCHS+1), training_accuracies, marker='o', linewidth=2, color='green', label='Training Accuracy')\n",
        "axes[1].axhline(y=accuracy_before, color='r', linestyle='--', label='Accuracy iniziale', linewidth=2)\n",
        "axes[1].set_xlabel('Epoca', fontsize=12)\n",
        "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "axes[1].set_title('Training Accuracy durante Fine-Tuning', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('finetuning_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Grafico salvato come 'finetuning_results.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81390804",
      "metadata": {
        "id": "81390804"
      },
      "outputs": [],
      "source": [
        "# Grafico comparativo Prima vs Dopo\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "categories = ['Loss', 'Accuracy (%)']\n",
        "before_values = [loss_before, accuracy_before]\n",
        "after_values = [loss_after, accuracy_after]\n",
        "\n",
        "x = range(len(categories))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar([i - width/2 for i in x], before_values, width,\n",
        "               label='Prima del fine-tuning', alpha=0.8, color='#FF6B6B')\n",
        "bars2 = ax.bar([i + width/2 for i in x], after_values, width,\n",
        "               label='Dopo il fine-tuning', alpha=0.8, color='#4ECDC4')\n",
        "\n",
        "ax.set_ylabel('Valore', fontsize=12)\n",
        "ax.set_title('Confronto Prestazioni: Prima vs Dopo Fine-Tuning', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(categories, fontsize=11)\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "# Aggiungi valori sopra le barre\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.2f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('comparison_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Grafico di confronto salvato come 'comparison_results.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d2b541f",
      "metadata": {
        "id": "6d2b541f"
      },
      "source": [
        "## üíæ Step 11: Salvataggio Modello Fine-Tuned\n",
        "\n",
        "Salviamo il modello fine-tuned per poterlo riutilizzare in futuro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "445e426d",
      "metadata": {
        "id": "445e426d"
      },
      "outputs": [],
      "source": [
        "# Path output\n",
        "output_path = \"resnet50_places365_finetuned.pth\"\n",
        "\n",
        "# Salva il modello con tutte le informazioni utili\n",
        "torch.save({\n",
        "    'epoch': NUM_EPOCHS,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss_before': loss_before,\n",
        "    'loss_after': loss_after,\n",
        "    'accuracy_before': accuracy_before,\n",
        "    'accuracy_after': accuracy_after,\n",
        "    'training_losses': training_losses,\n",
        "    'training_accuracies': training_accuracies,\n",
        "}, output_path)\n",
        "\n",
        "print(f\"‚úì Modello salvato in '{output_path}'\")\n",
        "print(f\"‚úì Dimensione file: {os.path.getsize(output_path) / (1024**2):.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ac2372",
      "metadata": {
        "id": "06ac2372"
      },
      "source": [
        "## üìù Step 12: Riepilogo Finale\n",
        "\n",
        "Ecco un riepilogo completo dell'esperimento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52981248",
      "metadata": {
        "id": "52981248"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ ESPERIMENTO COMPLETATO!\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "print(\"üìä RIEPILOGO:\")\n",
        "print(f\"  ‚Ä¢ Device utilizzato: {DEVICE}\")\n",
        "print(f\"  ‚Ä¢ Numero di epoche: {NUM_EPOCHS}\")\n",
        "print(f\"  ‚Ä¢ Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  ‚Ä¢ Batch size: {BATCH_SIZE}\")\n",
        "print()\n",
        "print(\"üìà RISULTATI:\")\n",
        "print(f\"  ‚Ä¢ Loss:     {loss_before:.4f} ‚Üí {loss_after:.4f} ({loss_before - loss_after:+.4f})\")\n",
        "print(f\"  ‚Ä¢ Accuracy: {accuracy_before:.2f}% ‚Üí {accuracy_after:.2f}% ({accuracy_after - accuracy_before:+.2f}%)\")\n",
        "print()\n",
        "print(\"üíæ FILE GENERATI:\")\n",
        "print(f\"  ‚Ä¢ {output_path} (modello fine-tuned)\")\n",
        "print(f\"  ‚Ä¢ finetuning_results.png (grafici training)\")\n",
        "print(f\"  ‚Ä¢ comparison_results.png (confronto risultati)\")\n",
        "print()\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXGsYj-REQvZ"
      },
      "id": "RXGsYj-REQvZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}